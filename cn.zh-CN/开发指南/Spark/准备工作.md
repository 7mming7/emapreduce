# 准备工作 {#concept_wnv_rvg_hfb .concept}

本文介绍Spark开发所需要的准备工作

## 安装 E-MapReduce SDK {#section_ys4_nwg_hfb .section}

你可以通过以下两种方法安装 E-MapReduce SDK:

-   方法一：直接在 Eclipse 中使用 JAR 包，步骤如下：
    1.  从Maven库源下载E-MapReduce开发的[依赖](http://mvnrepository.com/search?q=com.aliyun.emr)包。

    2.  将所需的jar包拷贝到您的工程文件夹中。（SDK版本是否选择2.10还是2.11主要根据您运行作业的集群Spark版本，Spark1.x版本建议使用2.10，Spark2.x建议使用2.11）

    3.  在 Eclipse 中右击工程的名字，然后点击**Properties** \> **Java Build Path** \> **Add JARs**。

    4.  选择您下载的 SDK。

    5.  经过上面几步之后，您就可以在工程中读写 OSS、LogService、MNS、ONS、OTS、MaxCompute 等数据了。

-   方法二：Maven 工程的方式，请添加如下依赖：

    ```
    <!--支持OSS数据源 -->
        <dependency>
            <groupId>com.aliyun.emr</groupId>
            <artifactId>emr-core</artifactId>
            <version>1.4.1</version>
        </dependency>
        <!--支持OTS数据源-->
        <dependency>
            <groupId>com.aliyun.emr</groupId>
            <artifactId>emr-tablestore</artifactId>
            <version>1.4.1</version>
        </dependency>
        <!-- 支持 MNS、ONS、LogService、MaxCompute数据源 (Spark 1.x环境)-->
        <dependency>
            <groupId>com.aliyun.emr</groupId>
            <artifactId>emr-mns_2.10</artifactId>
            <version>1.4.1</version>
        </dependency>
        <dependency>
            <groupId>com.aliyun.emr</groupId>
            <artifactId>emr-logservice_2.10</artifactId>
            <version>1.4.1</version>
        </dependency>
        <dependency>
            <groupId>com.aliyun.emr</groupId>
            <artifactId>emr-maxcompute_2.10</artifactId>
            <version>1.4.1</version>
        </dependency>
        <dependency>
            <groupId>com.aliyun.emr</groupId>
            <artifactId>emr-ons_2.10</artifactId>
            <version>1.4.1</version>
        </dependency>
        <!-- 支持 MNS、ONS、LogService、MaxCompute数据源 (Spark 2.x环境)-->
        <dependency>
            <groupId>com.aliyun.emr</groupId>
            <artifactId>emr-mns_2.11</artifactId>
            <version>1.4.1</version>
        </dependency>
        <dependency>
            <groupId>com.aliyun.emr</groupId>
            <artifactId>emr-logservice_2.11</artifactId>
            <version>1.4.1</version>
        </dependency>
        <dependency>
            <groupId>com.aliyun.emr</groupId>
            <artifactId>emr-maxcompute_2.11</artifactId>
            <version>1.4.1</version>
        </dependency>
        <dependency>
            <groupId>com.aliyun.emr</groupId>
            <artifactId>emr-ons_2.11</artifactId>
            <version>1.4.1</version>
        </dependency>
    ```


## Spark 代码本地调试 {#section_ugl_zwg_hfb .section}

spark.hadoop.mapreduce.job.run-local 这个配置项只是针对需要在本地调试 Spark 代码读写 OSS 数据的场景，除此之外只需要保持默认即可。

如果需要本地调试运行 Spark 代码读写 OSS 数据，则需要对 SparkConf 进行配置，将spark.hadoop.mapreduce.job.run-local设为true，例如如下代码：

```
val conf = new SparkConf().setAppName(getAppName).setMaster("local[4]")
   conf.set("spark.hadoop.fs.oss.impl", "com.aliyun.fs.oss.nat.NativeOssFileSystem")
   conf.set("spark.hadoop.mapreduce.job.run-local", "true")
   val sc = new SparkContext(conf) 
   val data = sc.textFile("oss://...")
   println(s"count: ${data.count()}")
```

## 三方依赖说明 {#section_m1y_wyg_hfb .section}

为了支持在E-MapReduce上操作阿里云的数据源（包括 OSS、MaxCompute等），需要您的作业依赖一些三方包。

您可以参照这个[pom](https://github.com/aliyun/aliyun-emapreduce-demo/blob/master/pom.xml)文件来增删需要依赖的三方包。

## OSS输出目录设置 {#section_axv_scv_vfb .section}

在本地的hadoop配置文件中增加 fs.oss.buffer.dirs 这个参数的配置，参数值是本地的目录路径。如果没有设置这个值，那么在本地运行Spark程序的时候，写OSS数据会碰到空指针的异常。

## 垃圾清理 {#section_rdp_yyg_hfb .section}

Spark作业失败后，已产生的数据是不会主动清理掉的。当您运行Spark作业失败后，请检查一下OSS输出目录是否有文件存在，您还需要检查OSS碎片管理中是否还有没有提交的碎片存在，如果存在请及时清理掉。

## pyspark 使用说明 {#section_qg3_zyg_hfb .section}

如果您使用的是python来编写Spark作业，那么请[参考这里](https://github.com/aliyun/aliyun-emapreduce-sdk/blob/master/docs/how_to_run_spark_with_python_sdk.md)配置您的环境。

