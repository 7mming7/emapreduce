# 准备工作 {#concept_wnv_rvg_hfb .concept}

本文介绍 Spark 开发所需要的准备工作

## 安装 E-MapReduce SDK {#section_ys4_nwg_hfb .section}

你可以通过以下两种方法安装 E-MapReduce SDK:

-   直接在 Eclipse 中使用 JAR 包，步骤如下：
    1.  从 Maven 库源下载 E-MapReduce 开发的[依赖](http://mvnrepository.com/search?q=com.aliyun.emr)包。

    2.  将所需的 JAR 包拷贝到您的工程文件夹中。（根据您运行作业的集群 Spark 版本选择 SDK 版本，Spark 1.x 版本建议使用 2.10，Spark 2.x 建议使用 2.11）

    3.  在 Eclipse 中工程的名字上单击右键，然后依次选择**Properties** \> **Java Build Path** \> **Add JARs**。

    4.  选择您下载的 SDK。

    5.  经过上面几步之后，您就可以在工程中读写 OSS、LogService、MNS、ONS、OTS、MaxCompute 等数据了。

-   Maven 工程的方式，请添加如下依赖：

    ```
    <!--支持OSS数据源 -->
        <dependency>
            <groupId>com.aliyun.emr</groupId>
            <artifactId>emr-core</artifactId>
            <version>1.4.1</version>
        </dependency>
        <!--支持OTS数据源-->
        <dependency>
            <groupId>com.aliyun.emr</groupId>
            <artifactId>emr-tablestore</artifactId>
            <version>1.4.1</version>
        </dependency>
        <!-- 支持 MNS、ONS、LogService、MaxCompute数据源 (Spark 1.x环境)-->
        <dependency>
            <groupId>com.aliyun.emr</groupId>
            <artifactId>emr-mns_2.10</artifactId>
            <version>1.4.1</version>
        </dependency>
        <dependency>
            <groupId>com.aliyun.emr</groupId>
            <artifactId>emr-logservice_2.10</artifactId>
            <version>1.4.1</version>
        </dependency>
        <dependency>
            <groupId>com.aliyun.emr</groupId>
            <artifactId>emr-maxcompute_2.10</artifactId>
            <version>1.4.1</version>
        </dependency>
        <dependency>
            <groupId>com.aliyun.emr</groupId>
            <artifactId>emr-ons_2.10</artifactId>
            <version>1.4.1</version>
        </dependency>
        <!-- 支持 MNS、ONS、LogService、MaxCompute数据源 (Spark 2.x环境)-->
        <dependency>
            <groupId>com.aliyun.emr</groupId>
            <artifactId>emr-mns_2.11</artifactId>
            <version>1.4.1</version>
        </dependency>
        <dependency>
            <groupId>com.aliyun.emr</groupId>
            <artifactId>emr-logservice_2.11</artifactId>
            <version>1.4.1</version>
        </dependency>
        <dependency>
            <groupId>com.aliyun.emr</groupId>
            <artifactId>emr-maxcompute_2.11</artifactId>
            <version>1.4.1</version>
        </dependency>
        <dependency>
            <groupId>com.aliyun.emr</groupId>
            <artifactId>emr-ons_2.11</artifactId>
            <version>1.4.1</version>
        </dependency>
    ```


## Spark 代码本地调试 {#section_ugl_zwg_hfb .section}

spark.hadoop.mapreduce.job.run-local 这个配置项只是针对需要在本地调试 Spark 代码读写 OSS 数据的场景，除此之外只需要保持默认即可。

如果需要本地调试运行 Spark 代码读写 OSS 数据，则需要对 SparkConf 进行配置，将spark.hadoop.mapreduce.job.run-local设为true，例如如下代码：

```
val conf = new SparkConf().setAppName(getAppName).setMaster("local[4]")
   conf.set("spark.hadoop.fs.oss.impl", "com.aliyun.fs.oss.nat.NativeOssFileSystem")
   conf.set("spark.hadoop.mapreduce.job.run-local", "true")
   val sc = new SparkContext(conf) 
   val data = sc.textFile("oss://...")
   println(s"count: ${data.count()}")
```

## 常见问题 {#section_vjt_twf_1xk .section}

-   三方依赖说明

    为了支持在 E-MapReduce 上操作阿里云的数据源（包括 OSS、MaxCompute 等），您的作业需要依赖一些三方包。

    您可以参照[pom](https://github.com/aliyun/aliyun-emapreduce-demo/blob/master/pom.xml) 文件来增删需要依赖的三方包。

-   OSS 输出目录设置

    在本地的 Hadoop 配置文件中增加 fs.oss.buffer.dirs 这个参数的配置，参数值是本地的目录路径。如果没有设置这个值，那么在本地运行 Spark 程序的时候，写 OSS 数据会碰到空指针的异常。

-   垃圾清理

    Spark作业失败后，已产生的数据是不会主动清理掉的。当您运行Spark作业失败后，请检查一下OSS输出目录是否有文件存在，您还需要检查OSS碎片管理中是否还有没有提交的碎片存在，如果存在请及时清理掉。

-   pyspark 使用说明

    如果您使用的是 python 来编写 Spark 作业，那么请[参考这里](https://github.com/aliyun/aliyun-emapreduce-sdk/blob/master/docs/how_to_run_spark_with_python_sdk.md)配置您的环境。


