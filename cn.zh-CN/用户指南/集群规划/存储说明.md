# 存储说明 {#concept_ift_2n3_y2b .concept}

在节点上存在2种角色的磁盘，一类是系统盘，用来安装操作系统。一类是数据盘，用来保存数据。系统盘默认都是一块，必须使用云盘。而数据盘可以有很多块，目前上限可以一个节点挂16块。每一块都可以有不同的配置，类型和容量都可以不同。EMR默认使用SSD云盘作为集群的系统盘。EMR 默认挂载4块云盘，目前的内网带宽的情况下4块云盘是比较合理的配置。

## 云盘与本地盘 {#section_flt_fn3_y2b .section}

有2种类型的磁盘可以用作数据的存储：

-   云盘

    包括，SSD 云盘，高效云盘，普通云盘。

    特点是，磁盘并不直接挂载在本地的计算节点上，通过网络访问远端的一个存储节点。每一份数据在后端都有2个实时备份，一共三份数据。所以当一份数据损坏的时候（磁盘损坏，不是用户自己的业务上的破坏），会自动的使用备份数据恢复。

-   本地盘

    包括，大数据型的 SATA 本地盘，和本地 SSD 的本地 SSD 盘。

    直接挂载在计算节点上的磁盘，拥有超过云盘的性能表现。使用本地盘的时候不能选择数量，只能使用默认配置好的数量，和线下物理机一样，数据没有后端的备份机制，需要上层的软件来保证数据可靠性。


## 适用的场景 {#section_srg_3n3_y2b .section}

在EMR中，所有云盘和本地盘都会在节点释放的时候清除数据，磁盘无法独立的保存下来，并再次使用。Hadoop HDFS 会使用所有的数据盘作为数据存储。 Hadoop YARN 也会使用所有的数据作为计算的临时存储。

当业务数据量并不太大（T级别以下）的时候，可以使用云盘，IOPS和吞吐相比本地盘都会小些。数据量大的时候，推荐都使用本地盘，EMR 会来维护本地盘的数据可靠性。如果发现在使用中明显的吞吐量不够用，可以切换到本地盘的存储上。

## OSS {#section_dpg_jn3_y2b .section}

在 EMR 中可以将 OSS 作为 HDFS 使用。 用户可以非常方便的读写OSS，所有使用 HDFS 的代码也可以简单的修改就能访问 OSS 上的数据了。

比如：

Spark中读取数据

```
sc.Textfile("hdfs://user/path")
```

替换存储类型 hdfs -\> oss

```
sc.Textfile("oss://user/path")
```

对于MR或者Hive作业也是一样

HDFS命令直接操作OSS数据

```
hadoop fs -ls oss://bucket/path
hadoop fs -cp hdfs://user/path  oss://bucket/path
```

这个过程，不需要输入AK和endpoint，EMR都会自动替用户使用当前集群所有者的信息补全。

但 OSS 的 iops 不高，在一些需要高 IOPS的场景，不适合使用，比如流式计算 Spark Streaming 或 HBase。

