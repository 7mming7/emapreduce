# 连接器（旧文档，不发布） {#concept_x1d_4zd_z2b .concept}

## 系统连接器 {#section_wc2_d12_z2b .section}

-   概述

    通过本连接器可以使用SQL查询Presto集群的基本信息和度量。

-   配置

    无需配置，所有信息都可以通过名为 system 的catalog获取。

    示例如下：

    ```
    --- 列出所有支持的数据项目
    SHOW SCHEMAS FROM system;
    --- 列出运行时项目中的所有数据项
    SHOW TABLES FROM system.runtime;
    --- 获取节点状态
    SELECT * FROM system.runtime.nodes;
         node_id |         http_uri         | node_version | coordinator | state
    -------------+--------------------------+--------------+-------------+--------
     3d7e8095-...| http://192.168.1.100:9090| 0.188        | false       | active
     7868d742-...| http://192.168.1.101:9090| 0.188        | false       | active
     7c51b0c1-...| http://192.168.1.102:9090| 0.188        | true        | active
    --- 强制取消一个查询
    CALL system.runtime.kill_query('20151207_215727_00146_tx3nr');
    ```

-   数据表

    本连接器提供了如下几个数据表：

    |TABLE|SCHEMA|说明|
    |:----|:-----|:-|
    |catalogs|metadata|该表包含了本连接器支持的所有catalog列表|
    |schema\_properties|metadata|本表包含创建新Schema时可以设置的可用属性列表。|
    |table\_properties|metadata|本表包含创建新表时可以设置的可用属性列表。|
    |nodes|runtime|本表包含了Presto集群中所有可见节点及其状态列表|
    |queries|runtime|查询表包含了Presto群集上当前和最近运行的查询的信息，包括原始查询文本（SQL），运行查询的用户的身份以及有关查询的性能信息，如查询排队和分析的时间等。|
    |tasks|runtime|任务表包含了关于Presto查询中涉及的任务的信息，包括它们的执行位置以及每个任务处理的行数和字节数。|
    |transactions|runtime|本表包含当前打开的事务和相关元数据的列表。这包括创建时间，空闲时间，初始化参数和访问目录等信息。|

-   存储过程

    本连接器支持如下存储过程：

    `runtime.kill_query(id)` 取消给定id的查询


## JMX连接器 {#section_hjf_512_z2b .section}

-   概述

    可以通过JMX连接器查询Presto集群中所有节点的JMX信息。本连接器通常用于系统监控和调试。通过修改本连接器的配置，可以实现JMX信息定期转储的功能。

-   配置

    创建 etc/catalog/jmx.properties 文件，添加如下内容，一边启用JMX连接器。

    ```
    connector.name=jmx
    ```

    如果希望定期转储JMX数据，可以在配置文件中添加如下内容：

    ```
    connector.name=jmx
    jmx.dump-tables=java.lang:type=Runtime,com.facebook.presto.execution.scheduler:name=NodeScheduler
    jmx.dump-period=10s
    jmx.max-entries=86400
    ```

    其中：

    -   dump-tables 是用逗号隔开的 MBean（Managed Beans）列表。该配置项指定了每个采样周期哪些MBean指标会被采样并存储到内存中。
    -   dump-period 用于设置采样周期，默认为10s。
    -   max-entries 用于设置历史记录的最大长度，默认为86400条。
    如果指标项的名称中包含逗号，则需要使用 `\,` 进行转义，如下所示：

    ```
    connector.name=jmx
    jmx.dump-tables=com.facebook.presto.memory:type=memorypool\\,name=general,\
       com.facebook.presto.memory:type=memorypool\\,name=system,\
       com.facebook.presto.memory:type=memorypool\\,name=reserved
    ```

-   数据表

    JMX连接器提供了两个 schemas，分别为 **current** 和 **history**。其中：

    **current** 中包含了Presto集群中每个节点的当前的MBean，BMean的名称即为 **current** 中的表名（如果bean的名称中包含非标准字符，则需在查询时用双引号将表名扩起来），可以通过如下语句获取：

    ```
    SHOW TABLES FROM jmx.current;
    ```

    示例如下：

    ```
    --- 获取每个节点的jvm信息
    SELECT node, vmname, vmversion
    FROM jmx.current."java.lang:type=runtime";
          node    |              vmname               | vmversion
    --------------+-----------------------------------+-----------
     ddc4df17-xxx | Java HotSpot(TM) 64-Bit Server VM | 24.60-b09
    (1 row)
    ```

    ```
    --- 获取每个节点最大和最小的文件描述符个数指标
    SELECT openfiledescriptorcount, maxfiledescriptorcount
    FROM jmx.current."java.lang:type=operatingsystem";
     openfiledescriptorcount | maxfiledescriptorcount
    -------------------------+------------------------
                         329 |                  10240
    (1 row)
    ```

    **history** 中包含了配置文件中配置的需要转储的指标对应的数据表。可以通过如下语句进行查询：

    ```
    SELECT "timestamp", "uptime" FROM jmx.history."java.lang:type=runtime";
            timestamp        | uptime
    -------------------------+--------
     2016-01-28 10:18:50.000 |  11420
     2016-01-28 10:19:00.000 |  21422
     2016-01-28 10:19:10.000 |  31412
    (3 rows)
    ```


## Kafka连接器 {#section_j2t_212_z2b .section}

-   概述

    本连接器将Kafka上的topic映射为Presto中的表。kafka中的每条记录都映射为Presto表中的消息。

    **说明：** 由于Kafka中，数据是动态变化的，因此，在使用presto进行多次查询时，有时候会出现一些比较奇怪的现象。目前，Presto还没有处理这些情况的能力。

-   配置

    创建 etc/catalog/kafka.properties 文件，添加如下内容，一边启用JMX连接器。

    ```
    connector.name=kafka
    kafka.table-names=table1,table2
    kafka.nodes=host1:port,host2:port
    ```

    **说明：** Presto可以同时连接多个Kafka集群，只需要在配置目录中添加新的properties文件即可，文件名将会被映射为Presto的catalog。如新增一个 orders.properties 配置文件，Presto会创建一个新的名为 orders 的catalog。

    ```
    ## orders.properties
    connector.name=kafka  # 这个表示连接器类型，不能变
    kafka.table-names=tableA,tableB
    kafka.nodes=host1:port,host2:port
    ```

    Kafka连接器提供了如下几个属性：

    -   kafka.table-names

        说明： 必选，定义本连接器支持的表格列表。

        详情： 这里的文件名可以使用Schema名称进行修饰，形式如\{schema\_name\}.\{table\_name\}。也可以不使用Schema名称修饰，此时，表格将被映射到 kafka.default-schema 中定义的schema中。

    -   kafka.default-schema

        说明： 可选，默认的Schema名称, 默认值为 default。

    -   kafka.nodes

        说明： 必选，Kafka集群中的节点列表。

        详情： 配置格式形如 `hostname:port[,hostname:port…]`。此处的可以只配置部分kafka节点，但是，Presto必须能够连接到kafka集群中的所有节点。否则，可能拿不到部分数据。

    -   kafka.connect-timeout

        说明： 可选，连接器与Kafka集群的超时时间，默认为10s。

        详情： 如果Kafka集群压力比较大，创建连接可能需要相当长的时间，从而导致Presto在执行查询时出现超时的情况。此时，增加当前的配置值是一个不错的选择。

    -   kafka.buffer-size

        说明： 可选，读缓冲区大小，默认为64kb。

        详情： 用于设置从Kafka读取数据的内部数据缓冲区的大小。 数据缓冲区必须至少大于一条消息的大小。 每个Worker和数据节点都会分配一个数据缓冲区。

    -   kafka.table-description-dir

        说明： 可选，topic（表）描述文件目录，默认为 etc/kafka。

        详情： 该目录下保存着JSON格式的数据表定义文件（必须使用 .json 作为后缀）。

    -   kafka.hide-internal-columns

        说明：可选，需要隐藏的预置列清单，默认为true。

        详情： 除了在表格描述文件中定义的数据列之外，连接器还为每个表格维护了许多额外的列。本属性用于控制这些列在是否会在 `DESCRIBE` 和 `SELECT *` 语句的执行结果中显示。无论本配置设置是什么，这些列都可以参与查询过程中。

    Kafka连接器提供的内部列如下表所示：

    |列名|类型|说明|
    |:-|:-|:-|
    |\_partition\_id|BIGINT|本行记录所在的Kafka分区ID。|
    |\_partition\_offset|BIGINT|本行记录在Kafka分区中的偏移量。|
    |\_segment\_start|BIGINT|包含此行的数据段的最低偏移量。此偏移量是针对每个分区的。|
    |\_segment\_end|BIGINT|包含此行的数据段的最大偏移量（为下一个段的起始偏移量）。此偏移量是针对每个分区的。|
    |\_segment\_count|BIGINT|当前行在数据段中的序号。对于没有压缩的topic，\_segment\_start + \_segment\_count = \_partition\_offset。|
    |\_message\_corrupt|BOOLEAN|如果解码器无法解码本行记录，本字段将会被设为 TRUE。|
    |\_message|VARCHAR|将消息字节作为UTF-8编码的字符串。当topic的消息为文本类型是，这个字段比较有用。|
    |\_message\_length|BIGINT|本行消息的字节长度。|
    |\_key\_corrupt|BOOLEAN|如果键解码器无法解码本行记录，该字段将会被设为TRUE。|
    |\_key|VARCHAR|将键字节作为UTF-8编码的字符串。当topic的消息为文本类型是，这个字段比较有用。|
    |\_key\_length|BIGINT|消息中键的字节长度。|

    **说明：** 对于那些没有定义文件的表，\_key\_corrupt 和 \_message\_corrupt 永远为 FALSE。

-   表格定义文件

    Kafka本身是Schema-Less的消息系统，消息的格式需要生产者和消费者自己来定义。而Presto要求数据必须可以被映射成表的形式。因此，通常需要用户根据消息的实际合适，提供对应的表格定义文件。对于JSON格式的消息，如果没有提供定义文件，也可以在查询时使用Presto的JSON函数进行解析。这种处理方式虽然比较灵活，但是对会增加SQL语句的编写难度。

    一个表格描述文件使用JSON来定义一个表，文件名可以任意，但是必须以 .json 作为扩展名。

    ```
    {
        "tableName": ...,
        "schemaName": ...,
        "topicName": ...,
        "key": {
            "dataFormat": ...,
            "fields": [
                ...
            ]
        },
        "message": {
            "dataFormat": ...,
            "fields": [
                ...
           ]
        }
    }
    ```

    |字段|可选性|类型|说明|
    |:-|:--|:-|:-|
    |tableName|必选|string|Presto表名|
    |schemaName|可选|string|表所在的Schema名称|
    |topicName|必选|string|Kafka topic名称|
    |key|可选|JSON object|消息键到列的映射规则|
    |message|可选|JSON object|消息到列的映射规则|

    其中，键和消息的映射规则使用如下几个字段来描述。

    |字段|可选性|类型|说明|
    |:-|:--|:-|:-|
    |dataFormat|必选|string|设置一组列的解码器|
    |fields|必选|JSON数组|列定义列表|

    这里的 fields 是一个JSON数组，每一个元素为如下的JOSN对象：

    ```
    {
        "name": ...,
        "type": ...,
        "dataFormat": ...,
        "mapping": ...,
        "formatHint": ...,
        "hidden": ...,
        "comment": ...
    }
    ```

    |字段|可选性|类型|说明|
    |:-|:--|:-|:-|
    |name|必选|string|列名|
    |type|必选|string|列的数据类型|
    |dataFormat|可选|string|列数据解码器|
    |mapping|可选|string|解码器参数|
    |formatHint|可选|string|设置在该列上的提示，可以被解码器使用|
    |hiddent|可选|boolean|是否隐藏|
    |comment|可选|string|列的描述|

-   解码器

    解码器的功能就是Kafka的消息（key+message）映射到数据表到列中。如果没有表的定义文件，Presto将使用**dummy**解码器。

    Kafka连接器提供了如下3中解码器：

    -   raw：不做转换，直接使用原始的bytes
    -   csv： 将消息作为CSV格式的字符串进行处理
    -   json：将消息作为JSON进行处理

